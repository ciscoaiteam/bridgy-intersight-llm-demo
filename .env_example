# Bridgy AI Assistant Environment Configuration File
# -----------------------------------------------
# Copy this file to '.env' in the project root directory:
#
# cp .env_example .env
#
# This is the ONLY location where the .env file should be located.

# LangSmith Configuration (for tracing and debugging)
# --------------------------------------------------
LANGSMITH_ENDPOINT="https://api.smith.langchain.com"
LANGSMITH_API_KEY=your_langsmith_api_key_here
LANGSMITH_PROJECT=bridgy
LANGSMITH_TRACING=true

# Intersight API credentials
# -------------------------
# API Key ID from your Intersight account
INTERSIGHT_API_KEY=your_intersight_api_key_here
# Path to your PEM file (absolute path recommended)
INTERSIGHT_SECRET_KEY_PATH=/path/to/your/secret/key.pem

# Nexus Dashboard credentials
# --------------------------
# URL to your Nexus Dashboard instance (include protocol and no trailing slash)
NEXUS_DASHBOARD_URL=https://your-nexus-dashboard-url
NEXUS_DASHBOARD_USERNAME=your_username
NEXUS_DASHBOARD_PASSWORD=your_password
NEXUS_DASHBOARD_DOMAIN=local

# LLM Configuration
# ----------------
# URL for the Ollama API if using local models
OLLAMA_API_URL=http://localhost:11434/api/chat
# Default model to use for the assistant
DEFAULT_MODEL=gemma2

# Remote LLM Service (if not using local Ollama)
# ---------------------------------------------
# Base URL for remote LLM service
LLM_BASE_URL=http://64.101.169.102:8000/v1
# Authentication for remote LLM service
LLM_API_KEY=llm-api-key
# Model path for remote LLM service
LLM_MODEL_PATH=/ai/models/Meta-Llama-3-8B-Instruct/