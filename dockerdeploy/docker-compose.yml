version: '3.8'

services:
  vllm-server:
    build:
      context: ../vllm
      dockerfile: Dockerfile
    image: bridgy-vllm-gemma2:latest
    container_name: bridgy-vllm-gemma2
    ports:
      - "8000:8000"
    environment:
      - HF_TOKEN=${HF_TOKEN:-}  # Pass your Hugging Face token if needed for model access
      - TENSOR_PARALLEL_SIZE=1  # Adjust based on available GPUs
    volumes:
      - ../vllm/models:/data/models  # Persist downloaded models
      - ../vllm/huggingface:/data/huggingface  # Persist HF cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    restart: unless-stopped

  bridgy-main:
    build:
      context: ..
      dockerfile: ../bridgy-main/Dockerfile
    ports:
      - "8443:8443"
    volumes:
      - ../config:/config
      - ../bridgy-main:/app/bridgy-main
      # Mount config for intersight key
      - ../config:/app/bridgy-main/configs
    environment:
      # Python & app settings
      - PYTHONPATH=/app:/app/bridgy-main:/app/bridgy:/tmp
      
      # MongoDB connection parameters
      - MONGODB_HOST=mongodb
      - MONGODB_PORT=27017
      - MONGODB_URI=mongodb://mongodb:27017/bridgy_db
      - MONGO_ENABLED=true
      
      # LLM configuration - choose between remote or local vLLM
      # Remote LLM service
      # - LLM_SERVICE_URL=http://64.101.169.102:8000/v1
      # - LLM_MODEL=/ai/models/Meta-Llama-3-8B-Instruct/
      # - LLM_API_KEY=${LLM_API_KEY:-llm-api-key}
      # Local vLLM service with Gemma 2
      - LLM_SERVICE_URL=http://vllm-server:8000/v1
      - LLM_MODEL=gemma-2-9b
      - LLM_API_KEY=not-needed
      
      # CUDA configuration
      - CUDA_VISIBLE_DEVICES=0
      
      # Nexus Dashboard integration
      - NEXUS_DASHBOARD_URL=${NEXUS_DASHBOARD_URL}
      - NEXUS_DASHBOARD_USERNAME=${NEXUS_DASHBOARD_USERNAME:-admin}
      - NEXUS_DASHBOARD_PASSWORD=${NEXUS_DASHBOARD_PASSWORD}
      
      # Intersight API integration
      - INTERSIGHT_API_KEY=${INTERSIGHT_API_KEY}
      - INTERSIGHT_PRIVATE_KEY_FILE=/app/bridgy-main/configs/intersight_api_key.pem
      
      # Langsmith configuration
      - LANGSMITH_API_KEY=${LANGSMITH_API_KEY}
      - LANGCHAIN_PROJECT=bridgy
    restart: unless-stopped
    depends_on:
      - mongodb
      - vllm-server
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  mongodb:
    image: mongo:6.0
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    restart: unless-stopped

  bridgy-frontend:
    build:
      context: ../bridgy-frontend
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    depends_on:
      - bridgy-main
    restart: unless-stopped

volumes:
  mongodb_data:
